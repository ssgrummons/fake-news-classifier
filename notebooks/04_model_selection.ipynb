{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8c4f1a",
   "metadata": {},
   "source": [
    "# Model Experimentation and Selection\n",
    "\n",
    "Given our initial success with fine-tuning BERT, we are going to attempt to start modularizing components and setting up our experiments pipeline.  This notebook is our initial attempt to start building out the reusable components for our Machine Learning pipelines and to start experimenting with MLFlow.\n",
    "\n",
    "After running some of these tests in notebooks, it is time to pivot to configuration-based modular code, as we will need to run multiple experiments using reproducible mechanisms with auditability and results tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.data.preparation import prepare_dataset_from_csv\n",
    "from src.features.tokenization import tokenizer, id2label, label2id\n",
    "from src.models.metrics import compute_metrics\n",
    "from src.mflow.experiment_tracking import start_run, log_params, log_metrics, log_model_from_checkpoint\n",
    "from src.utils.helpers import md_print\n",
    "\n",
    "# In order to run this effectively on a MacBook, I needed to leverage the below Environment Variable.\n",
    "#\n",
    "# TOKENIZERS_PARALLELISM=False\n",
    "#\n",
    "# So I added a .env file to the ./notebooks directory and set the variable there.  \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec88940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = prepare_dataset_from_csv(\"../data/raw/train.csv\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Validate all classes are represented equally across the train and eval sets.  \n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(train_dataset[\"labels\"]))\n",
    "print(Counter(eval_dataset[\"labels\"]))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\",\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# Enable gradient checkpointing to save memory during training.  \n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a80d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "formatted_date = now.strftime(\"%Y-%m-%d_%H:%M\")\n",
    "\n",
    "with start_run(run_name=f\"distilbert_hyperparam_trial_{formatted_date}\"):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"../results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=[],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        fp16=False,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    log_params(vars(training_args))  # log the full TrainingArguments config\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    log_metrics(eval_metrics)\n",
    "    log_model_from_checkpoint(model=trainer.model, tokenizer=tokenizer)\n",
    "    \n",
    "    md_print(\"### Final evaluation metrics:\")\n",
    "    for k, v in eval_metrics.items():\n",
    "        md_print(f\"  **{k}:** {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
