{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf04a83",
   "metadata": {},
   "source": [
    "# Baseline ML Modeling with Transformers\n",
    "\n",
    "This notebook will attempt to use the Transformers library to classify the author who wrote a given text.  \n",
    "\n",
    "## Transformer Models\n",
    "Transformer models are better at semantic understanding and unpacking semantic meaning from vectorized representations of text.  So we will run an initial experiment with a BERT based model using the Hugging Face Transformers library to see if it outperforms our Logisitc Regression TF-IDF model.  If it can achieve a 90% F1 score, then we can build an experiment pipeline to identify the right combination of data preparation, model selection, and hyperparameters to achieve the highest score.\n",
    "\n",
    "## Optimization Metric\n",
    "This is a multi-class problem with a business requirement to balance precision and recall.  Given that our classes are balanced in our training data set, we can take the harmonic mean of precision and recall and weight it equally.  So we will optimize our model for _F1 Macro_, though we will still track all metrics through this training cycle to ensure that continues to make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f033928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    "from datasets import Dataset, ClassLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "label2id = {'dickens': 0,\n",
    "              'doyle': 1,\n",
    "              'twain': 2,\n",
    "              'defoe': 3}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def preprocess_function(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Preprocess the input data for training. \n",
    "    This function tokenizes the text and maps the author names to their corresponding IDs. \n",
    "    It also truncates the text to a maximum length of 64 tokens and pads it with zeros if necessary. \n",
    "    The resulting dataset is then converted into a format suitable for training using the Transformers library.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary containing the text and author name of a single example.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed text and corresponding author ID.\n",
    "    \"\"\"\n",
    "    return tokenizer(example[\"text\"], \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\",\n",
    "                     max_length=64)\n",
    "\n",
    "def prepare_dataset_from_csv(csv_path: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Prepare a Hugging Face dataset from a CSV file containing text and author names.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): The path to the CSV file containing text and author names.\n",
    "    Returns:\n",
    "        Dataset: A Hugging Face dataset containing preprocessed text and corresponding author ID.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[[\"text\", \"author\"]]\n",
    "    df['labels'] = df['author'].map(label2id)\n",
    "\n",
    "    # Convert to HF Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Cast labels to ClassLabel\n",
    "    class_label = ClassLabel(num_classes=4, names=list(label2id.keys()))\n",
    "    hf_dataset = hf_dataset.cast_column(\"labels\", class_label)\n",
    "\n",
    "    # Tokenize\n",
    "    hf_dataset = hf_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    return hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81393fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the dataset to a fixed length.  \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset = prepare_dataset_from_csv('../data/raw/train.csv')\n",
    "\n",
    "split_dataset = prepared_dataset.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Validate all classes are represented equally across the train and eval sets.  \n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(train_dataset[\"labels\"]))\n",
    "print(Counter(eval_dataset[\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from scipy.special import softmax\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    probs = softmax(logits, axis=1)  # For log loss\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision_macro\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"log_loss\": log_loss(labels, probs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77274a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=4, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", \n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
